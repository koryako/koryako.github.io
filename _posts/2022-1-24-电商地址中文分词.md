---
layout: post
title: "地址分词"
subtitle: '中文地址分词，ner'
author: "koryako"
header-style: text
tags:
  - 地址分词
---

地址分词
---



https://github.com/geeklili/albert-finetune



google 的bert 预训练模型[bert 源码解读] https://www.jianshu.com/p/3d0bb34c488a   ,她的中文变体 albert 和roberta都是在其源码的基础上改造而成，并使用中文语料重新训练而成

我们使用 [基于BERT预训练的中文命名实体识别TensorFlow实现](https://blog.csdn.net/macanv/article/details/85684284)[源码](https://github.com/macanv/BERT-BiLSTM-CRF-NER)  进行实际的部署， 他的tf2.0 代码  可以[参考 2.0 代码](https://github.com/StanleyLsx/entity_extractor_by_ner) 




1. https://github.com/google-research/bert#fine-tuning-with-bert （谷歌开源BERT）

2. https://blog.csdn.net/macanv/article/details/85684284 （基于BERT预训练的中文命名实体识别TensorFlow实现）

3. https://www.jianshu.com/p/b05e50f682dd（序列标注——实体识别BERT-BLSTM-CRF）

4. https://www.jianshu.com/p/3d0bb34c488a（BERT的demo运行）

5. https://python.ctolib.com/ProHiryu-albert-chinese-ner.html（使用预训练语言模型ALBERT做中文NER）

6. https://github.com/kyzhouhzau/BERT-NER（输出带acc）

7. https://github.com/brightmart/albert_zh（ALBert 预训练模型）

8. https://github.com/macanv/BERT-BiLSTM-CRF-NER（BERT-BiLSTM-CRF-NER常用版）

9. https://www.jianshu.com/p/1d6689851622（五分钟搭建一个基于BERT的NER模型）

10. https://github.com/BrikerMan/Kashgari（快速搭建NER框架）

11. https://github.com/grallage/ALBERT-BiLSTM-CRF-NER（ALBERT-BiLSTM-CRF-NER）


bert [预训练语言理解问题](https://github.com/brightmart/bert_language_understanding)
[原始bert](https://github.com/google-research/bert)   [技术分析](https://zhuanlan.zhihu.com/p/49271699)  bert [发展史 ](https://zhuanlan.zhihu.com/p/65470719)

https://github.com/namisan/mt-dnn

https://github.com/rasbt/deeplearning-models

[Bert时代的创新：Bert在NLP各领域的应用进展](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650763568&idx=4&sn=f595b6783d9adf597273747acdcca910&chksm=871ab54eb06d3c58285276cc4c9cc8525283050623e93c1803974918a40654bbf19c9314ce29&mpshare=1&scene=1&srcid=#rd])

[多标签分类](https://github.com/hellonlp/classifier_multi_label)

[Ideas from google's bert for language understanding: Pre-train TextCNN](https://github.com/brightmart/bert_language_understanding)

[使用Bert预训练模型文本分类（内附源码）](https://www.jiqizhixin.com/articles/2019-03-13-4 )


#https://github.com/google-research/football
#https://github.com/mnicnc404/CartoonGan-tensorflow
#https://saic-violet.github.io/texturedavatar/
#http://densepose.org/
#https://arxiv.org/abs/1905.08776
#!pip install bert-tensorflow
#https://blog.csdn.net/broccoli2/article/details/108952022?utm_source=app 保存为pb

#!pip install bert-serving-server # server

#!pip install bert-serving-client # client, independent of 'bert-serving-server'
import bert
from bert import run_classifier
from bert import optimization
from bert import tokenization
#载入依赖
import pandas as pd
import os
import csv
import time
import datetime
import random
import json
import warnings
from collections import Counter
from math import sqrt
import gensim
import numpy as np
import tensorflow as tf
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
from keras.models import Sequential
from keras import layers

